
# 🌌 Spatial Scholar — AI at Meta Hackathon

[![Watch the demo](https://img.youtube.com/vi/8RkEchuDLX4/maxresdefault.jpg)](https://youtube.com/shorts/8RkEchuDLX4)

## 🚀 Mission

We’re bringing passion and curiosity back to learning by transforming abstract topics into immersive, explorable spatial experiences. Our belief is simple:

The hard parts of learning — breaking down concepts, organizing knowledge, and visualizing relationships — should happen autonomously.

⸻

## 🎯 The Problem

Learning is still stuck in 2D — PDFs, videos, flashcards, and chatbot windows. This makes it hard for learners to:
	•	Visualize abstract concepts
	•	Understand how topics connect
	•	Stay engaged long enough to retain information
	•	Get explanations tailored to their mental model

AI tutors exist — but they still live on flat screens. There’s no spatial memory, no immersion, and no embodied understanding.

⸻

## ✅ Our Solution

We built an AI-powered spatial learning assistant for Meta Quest that turns any spoken request into an interactive 3D knowledge map—automatically.

## 🧠 How It Works
	1.	Voice Input
The learner says something like:
“I need help learning binary trees.”
“Teach me BFS and DFS.”
	2.	LLM Understanding (Llama Integration)
The system sends the request to a structured Llama prompt.
	3.	Autonomous Topic Breakdown
The LLM generates:
	•	✅ A spoken explanation
	•	✅ A structured JSON concept graph (nodes + relationships)
	4.	3D Rendering in VR
The concept graph is visualized as floating, interactive nodes in the user’s passthrough space.
	5.	Speech + Text Output
Explanations are spoken aloud and stored for reuse or chaining.

⸻

## 🧩 Tech Stack

Hardware
	•	Meta Quest 3

Core Technologies
	•	Unity (C#)
	•	Meta XR SDK (Passthrough, Voice, AI Blocks)
	•	LlmAgent, SpeechToTextAgent, TextToSpeechAgent
	•	Custom JSON Graph Parsing & Rendering

AI
	•	Llama (via Meta or Hugging Face provider)
	•	JSON-based knowledge graph generation

Visualization
	•	3D Nodes & Edges rendered dynamically
	•	Anchored in passthrough camera space

⸻

## 🔥 Why It’s Different

This isn’t VR as a gimmick, it uses spatial computing where it’s strongest:
	•	🧠 Spatial memory & embodied cognition
	•	🌐 Topic mapping in 3D space
	•	🎙️ Voice-driven knowledge generation
	•	🛠️ Autonomous visualization of abstract ideas

We don’t force learners to adapt — the system adapts to them.

⸻

## 📌 Example Use Cases
	•	Interactive explanations of LeetCode algorithms
	•	Physics and math visual breakdowns
	•	Biology & anatomy relationships
	•	Language learning mind maps
	•	Onboarding / training modules

⸻

## ✅ MVP Features (Current)
	•	✅ Voice-based topic requests
	•	✅ LLM explanations (speech + text)
	•	✅ JSON concept graph generation
	•	✅ 3D node/edge visualization
	•	✅ Interactive, expandable layout

⸻

🎯 Next Steps
	• Integration with student learning services like canvas and google calendar
	•	Hand-based node expansion
	•	Visual subtopics/diagrams generated
	•	Persistent “knowledge rooms”
	•	Multi-user sessions
	•	Interactive assessments

⸻

🤝 Team Statement

We believe learning should feel like exploration, not obligation.
By combining AI and spatial computing, we’re making knowledge something you can see, move through, and remember.
