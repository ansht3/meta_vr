
# ğŸŒŒ Spatial Scholar â€” AI at Meta Hackathon

[![Watch the demo](https://img.youtube.com/vi/8RkEchuDLX4/maxresdefault.jpg)](https://youtube.com/shorts/8RkEchuDLX4)

## ğŸš€ Mission

Weâ€™re bringing passion and curiosity back to learning by transforming abstract topics into immersive, explorable spatial experiences. Our belief is simple:

The hard parts of learning â€” breaking down concepts, organizing knowledge, and visualizing relationships â€” should happen autonomously.

â¸»

## ğŸ¯ The Problem

Learning is still stuck in 2D â€” PDFs, videos, flashcards, and chatbot windows. This makes it hard for learners to:
	â€¢	Visualize abstract concepts
	â€¢	Understand how topics connect
	â€¢	Stay engaged long enough to retain information
	â€¢	Get explanations tailored to their mental model

AI tutors exist â€” but they still live on flat screens. Thereâ€™s no spatial memory, no immersion, and no embodied understanding.

â¸»

## âœ… Our Solution

We built an AI-powered spatial learning assistant for Meta Quest that turns any spoken request into an interactive 3D knowledge mapâ€”automatically.

## ğŸ§  How It Works
	1.	Voice Input
The learner says something like:
â€œI need help learning binary trees.â€
â€œTeach me BFS and DFS.â€
	2.	LLM Understanding (Llama Integration)
The system sends the request to a structured Llama prompt.
	3.	Autonomous Topic Breakdown
The LLM generates:
	â€¢	âœ… A spoken explanation
	â€¢	âœ… A structured JSON concept graph (nodes + relationships)
	4.	3D Rendering in VR
The concept graph is visualized as floating, interactive nodes in the userâ€™s passthrough space.
	5.	Speech + Text Output
Explanations are spoken aloud and stored for reuse or chaining.

â¸»

## ğŸ§© Tech Stack

Hardware
	â€¢	Meta Quest 3

Core Technologies
	â€¢	Unity (C#)
	â€¢	Meta XR SDK (Passthrough, Voice, AI Blocks)
	â€¢	LlmAgent, SpeechToTextAgent, TextToSpeechAgent
	â€¢	Custom JSON Graph Parsing & Rendering

AI
	â€¢	Llama (via Meta or Hugging Face provider)
	â€¢	JSON-based knowledge graph generation

Visualization
	â€¢	3D Nodes & Edges rendered dynamically
	â€¢	Anchored in passthrough camera space

â¸»

## ğŸ”¥ Why Itâ€™s Different

This isnâ€™t VR as a gimmick, it uses spatial computing where itâ€™s strongest:
	â€¢	ğŸ§  Spatial memory & embodied cognition
	â€¢	ğŸŒ Topic mapping in 3D space
	â€¢	ğŸ™ï¸ Voice-driven knowledge generation
	â€¢	ğŸ› ï¸ Autonomous visualization of abstract ideas

We donâ€™t force learners to adapt â€” the system adapts to them.

â¸»

## ğŸ“Œ Example Use Cases
	â€¢	Interactive explanations of LeetCode algorithms
	â€¢	Physics and math visual breakdowns
	â€¢	Biology & anatomy relationships
	â€¢	Language learning mind maps
	â€¢	Onboarding / training modules

â¸»

## âœ… MVP Features (Current)
	â€¢	âœ… Voice-based topic requests
	â€¢	âœ… LLM explanations (speech + text)
	â€¢	âœ… JSON concept graph generation
	â€¢	âœ… 3D node/edge visualization
	â€¢	âœ… Interactive, expandable layout

â¸»

ğŸ¯ Next Steps
	â€¢ Integration with student learning services like canvas and google calendar
	â€¢	Hand-based node expansion
	â€¢	Visual subtopics/diagrams generated
	â€¢	Persistent â€œknowledge roomsâ€
	â€¢	Multi-user sessions
	â€¢	Interactive assessments

â¸»

ğŸ¤ Team Statement

We believe learning should feel like exploration, not obligation.
By combining AI and spatial computing, weâ€™re making knowledge something you can see, move through, and remember.
